{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import text\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib notebook\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing as preproc\n",
    "from sklearn.feature_extraction import text\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLogisticRegression:\n",
    "    \n",
    "    def __init__(self, learning_rate = 1, num_iterations = 2000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.w = []\n",
    "        self.b = 0\n",
    "        \n",
    "    def initialize_weight(self,dim):\n",
    "        \"\"\"\n",
    "        This function creates a vector of zeros of shape (dim, 1)      for w and initializes b to 0.\n",
    "        Argument:\n",
    "        dim -- size of the w vector we want (or number of parameters  in this case)\n",
    "        \"\"\"\n",
    "        w = np.zeros((dim,1))\n",
    "        b = 0\n",
    "        return w, b\n",
    "\n",
    "    def sigmoid(self,z):\n",
    "        \"\"\"\n",
    "        Compute the sigmoid of z\n",
    "        Argument:\n",
    "        z -- is the decision boundary of the classifier\n",
    "        \"\"\"\n",
    "        s = 1/(1 + np.exp(-z)) \n",
    "        return s\n",
    "    \n",
    "    def hypothesis(self,w,X,b):\n",
    "        \"\"\"\n",
    "        This function calculates the hypothesis for the present model\n",
    "        Argument:\n",
    "         w -- weight vector\n",
    "         X -- The input vector\n",
    "         b -- The bias vector\n",
    "        \"\"\"\n",
    "        H = self.sigmoid(np.dot(w.T,X)+b) \n",
    "        return H\n",
    "    \n",
    "    def cost(self,H,Y,m):\n",
    "        \"\"\"\n",
    "        This function calculates the cost of hypothesis\n",
    "        Arguments: \n",
    "         H -- The hypothesis vector \n",
    "         Y -- The output \n",
    "         m -- Number training samples\n",
    "        \"\"\"\n",
    "        cost = -np.sum(Y*np.log(H)+ (1-Y)*np.log(1-H))/m \n",
    "        cost = np.squeeze(cost)   \n",
    "        return cost\n",
    "    \n",
    "    def cal_gradient(self, w,H,X,Y):\n",
    "        \"\"\"\n",
    "        Calculates gradient of the given model in learning space\n",
    "        \"\"\"\n",
    "        m = X.shape[1]\n",
    "        dw = np.dot(X,(H-Y).T)/m\n",
    "        db = np.sum(H-Y)/m\n",
    "        grads = {\"dw\": dw,\n",
    "                 \"db\": db}\n",
    "        return grads\n",
    " \n",
    "    def gradient_position(self, w, b, X, Y):\n",
    "        \"\"\"\n",
    "        It just gets calls various functions to get status of learning model\n",
    "        Arguments:\n",
    "         w -- weights, a numpy array of size (no. of features, 1)\n",
    "         b -- bias, a scalar\n",
    "         X -- data of size (no. of features, number of examples)\n",
    "         Y -- true \"label\" vector (containing 0 or 1 ) of size (1, number of examples)\n",
    "        \"\"\"\n",
    "  \n",
    "        m = X.shape[1]\n",
    "        H = self.hypothesis(w,X,b)         # compute activation\n",
    "        cost = self.cost(H,Y,m)               # compute cost\n",
    "        grads = self.cal_gradient(w, H, X, Y) # compute gradient\n",
    "        \n",
    "        return grads, cost\n",
    "    \n",
    "    def gradient_descent(self, w, b, X, Y, print_cost = False):\n",
    "        \"\"\"\n",
    "        This function optimizes w and b by running a gradient descent algorithm\n",
    "\n",
    "        Arguments:\n",
    "        w — weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "        b — bias, a scalar\n",
    "        X -- data of size (no. of features, number of examples)\n",
    "        Y -- true \"label\" vector (containing 0 or 1 ) of size (1, number of examples)\n",
    "        print_cost — True to print the loss every 100 steps\n",
    "\n",
    "        Returns:\n",
    "        params — dictionary containing the weights w and bias b\n",
    "        grads — dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "        costs — list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "        \"\"\"\n",
    " \n",
    "        costs = []\n",
    " \n",
    "        for i in range(self.num_iterations):\n",
    "        # Cost and gradient calculation \n",
    "            grads, cost = self.gradient_position(w,b,X,Y)\n",
    " \n",
    " \n",
    "            # Retrieve derivatives from grads\n",
    "            dw = grads['dw']\n",
    "            db = grads['db']\n",
    " \n",
    "            \n",
    "            # update rule \n",
    "            w = w - (self.learning_rate * dw) \n",
    "            b = b - (self.learning_rate * db)\n",
    " \n",
    "            # Record the costs\n",
    "            if i % 100 == 0:\n",
    "                costs.append(cost)\n",
    " \n",
    "            # Print the cost every 100 training iterations\n",
    "            if print_cost and i % 100 == 0:\n",
    "                 print ('Cost after iteration %i: %f' %(i, cost))\n",
    " \n",
    " \n",
    "        params = {'w': w,\n",
    "                  'b': b}\n",
    " \n",
    "        grads = {'dw': dw,\n",
    "                 'db': db}\n",
    " \n",
    "        return params, grads, costs\n",
    "\n",
    "    def predict(self,X):\n",
    "        '''\n",
    "        Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "\n",
    "        Arguments:\n",
    "        w -- weights, a numpy array of size (n, 1)\n",
    "        b -- bias, a scalar\n",
    "        X -- data of size (num_px * num_px * 3, number of examples)\n",
    "\n",
    "        Returns:\n",
    "        Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "        '''\n",
    "        \n",
    "        X = np.array(X)\n",
    "        m = X.shape[1]\n",
    "  \n",
    "        Y_prediction = np.zeros((1,m))\n",
    "  \n",
    "        w = self.w.reshape(X.shape[0], 1)\n",
    "        b = self.b\n",
    "        # Compute vector \"H\" \n",
    "        H = self.hypothesis(w, X, b)\n",
    " \n",
    "        for i in range(H.shape[1]):\n",
    "        # Convert probabilities H[0,i] to actual predictions p[0,i]\n",
    "            if H[0,i] >= 0.5:\n",
    "                Y_prediction[0,i] = 1\n",
    "            else: \n",
    "                Y_prediction[0,i] = 0\n",
    "   \n",
    "        return Y_prediction\n",
    "\n",
    "    def train_model(self, X_train, Y_train, X_test, Y_test, print_cost = False):\n",
    "        \"\"\"\n",
    "        Builds the logistic regression model by calling the function you’ve implemented previously\n",
    "\n",
    "        Arguments:\n",
    "        X_train — training set represented by a numpy array of shape (features, m_train)\n",
    "        Y_train — training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "        X_test — test set represented by a numpy array of shape (features, m_test)\n",
    "        Y_test — test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "        print_cost — Set to true to print the cost every 100 iterations\n",
    "\n",
    "        Returns:\n",
    "        d — dictionary containing information about the model.\n",
    "        \"\"\"\n",
    "        # initialize parameters with zeros \n",
    "        dim = np.shape(X_train)[0]\n",
    "        w, b = self.initialize_weight(dim)\n",
    "        # Gradient descent \n",
    "        parameters, grads, costs = self.gradient_descent(w, b, X_train, Y_train, print_cost = False)\n",
    " \n",
    "        # Retrieve parameters w and b from dictionary “parameters”\n",
    "        self.w = parameters['w']\n",
    "        self.b = parameters['b']\n",
    " \n",
    "        # Predict test/train set examples \n",
    "        Y_prediction_test = self.predict(X_test)\n",
    "        Y_prediction_train = self.predict(X_train)\n",
    "        # Print train/test Errors\n",
    "        train_score = 100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100\n",
    "        test_score = 100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100\n",
    "        print('train accuracy: {} %'.format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "        print('test accuracy: {} %'.format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "        d = {'costs': costs,\n",
    "             'Y_prediction_test': Y_prediction_test, \n",
    "             'Y_prediction_train' : Y_prediction_train, \n",
    "             'w' : self.w, \n",
    "             'b' : self.b,\n",
    "             'learning_rate': self.learning_rate,\n",
    "             'num_iterations': self.num_iterations,\n",
    "             'train accuracy': train_score,\n",
    "             'test accuracy': test_score}\n",
    " \n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing on a small dataset\n",
    "#Dataset\n",
    "X_train = np.array([[5,6,1,3,7,4,10,1,2,0,5,3,1,4],[1,2,0,2,3,3,9,4,4,3,6,5,3,7]])\n",
    "Y_train = np.array([[0,0,0,0,0,0,0,1,1,1,1,1,1,1]])\n",
    "X_test  = np.array([[2,3,3,3,2,4],[1,1,0,7,6,5]])\n",
    "Y_test  = np.array([[0,0,0,1,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 100.0 %\n",
      "test accuracy: 100.0 %\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "clf = MyLogisticRegression()\n",
    "d = clf.train_model(X_train, Y_train, X_test, Y_test)\n",
    "print (d[\"train accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Считать данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/i.pile/Downloads/archive-5/Reviews.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset={'UserId', 'ProfileName', 'Time', 'Text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'] = 0\n",
    "df.loc[df['Score'] > 3, ['Label']] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Доля нормальных отзывов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7794624974297659"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Label.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Накопали сокращения http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):\n",
    "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    # remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    # Tokenize each word\n",
    "    text =  nltk.WordPunctTokenizer().tokenize(text)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afcb2bd33fde49928494f2239a6259ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/393933 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['Text_Cleansed'] = df.Text.swifter.apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e98f8bd96c3400686459ec1b13387ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/393933 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lemm = nltk.stem.WordNetLemmatizer()\n",
    "df['lemmatized_text'] = df.Text_Cleansed.swifter.apply(lambda words: list(map(lemm.lemmatize, words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Мешок слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-7f3867409e6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbow_converter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbow_converter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Text_Cleansed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbow_converter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1336\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1338\u001b[0;31m         \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfeature_idx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_counter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                         \u001b[0mfeature_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mfeature_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bow_converter = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "x = bow_converter.fit_transform(df['Text_Cleansed'])\n",
    "\n",
    "words = bow_converter.get_feature_names()\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, test_data = sklearn.model_selection.train_test_split(df, train_size = 0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_transform = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[3,3], lowercase=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_bow = bow_transform.fit_transform(training_data['Text_Cleansed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te_bow = bow_transform.transform(test_data['Text_Cleansed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = training_data['Label']\n",
    "y_te = test_data['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_tr_bow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-259a69bbcfde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtfidf_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTfidfTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_tr_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_transform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr_bow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_tr_bow' is not defined"
     ]
    }
   ],
   "source": [
    "tfidf_transform = text.TfidfTransformer(norm=None)\n",
    "X_tr_tfidf = tfidf_transform.fit_transform(X_tr_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_logistic_classify(X_tr, y_tr, X_test, y_test, description, _C=1.0):\n",
    "    model = LogisticRegression(C=_C).fit(X_tr, y_tr)\n",
    "    score = model.score(X_test, y_test)\n",
    "    print('Test Score with', description, 'features', score)\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
